{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "with open('train.pkl', 'rb') as file:\n",
    "    train_data_raw = pickle.load(file)\n",
    "with open('valid.pkl', 'rb') as file:\n",
    "    valid_data_raw = pickle.load(file)\n",
    "with open('test.pkl', 'rb') as file:\n",
    "    test_data_raw = pickle.load(file)\n",
    "def data_processing(raw_data, index):\n",
    "    new_index = []\n",
    "    for i in range(12):\n",
    "        if i not in index:\n",
    "            new_index.append(i)\n",
    "    new_data = []\n",
    "    p_index = {}\n",
    "    for k in range(len(new_index)):\n",
    "        p_index[new_index[k]] = new_index[k] - k\n",
    "    for j in raw_data:\n",
    "        if j[-2] in new_index:\n",
    "            j[-2] -= p_index[j[-2]]\n",
    "            new_data.append(j)\n",
    "    return new_data\n",
    "index = []\n",
    "train_data_raw1 = data_processing(train_data_raw, index)\n",
    "valid_data_raw1 = data_processing(valid_data_raw, index)\n",
    "train_data = torch.tensor(train_data_raw1, dtype=torch.float)[:, :-2]\n",
    "valid_data = torch.tensor(valid_data_raw1, dtype=torch.float)[:, :-2]\n",
    "train_label = torch.tensor(train_data_raw1, dtype=torch.long)[:, -2]\n",
    "valid_label = torch.tensor(valid_data_raw1, dtype=torch.long)[:, -2]\n",
    "test_data = torch.tensor(test_data_raw, dtype=torch.float)[:, :-2]\n",
    "test_label = torch.tensor(test_data_raw, dtype=torch.long)[:, -1]\n",
    "estimator = LogisticRegression()  # 逻辑回归\n",
    "selector = RFE(estimator, 400, step=300)\n",
    "selector = selector.fit(train_data, train_label)\n",
    "  # True的特征就是最终得到的特征\n",
    "train_data = train_data[:, selector.support_]\n",
    "valid_data = valid_data[:, selector.support_]\n",
    "test_data = test_data[:, selector.support_]\n",
    "\n",
    "class My_dataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        super(My_dataset).__init__()\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.label[index]\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def make_weights_for_balanced_classes(images, nclasses):                     \n",
    "    count = [0] * nclasses                                                      \n",
    "    for item in images:                                                         \n",
    "        count[item] += 1                                                     \n",
    "    weight_per_class = [0.] * nclasses                                      \n",
    "    N = float(sum(count))                                                   \n",
    "    for i in range(nclasses):    \n",
    "        if i == 3:                                               \n",
    "            weight_per_class[i] = N/float(count[i]) / 15.0\n",
    "        elif i == 1:\n",
    "            weight_per_class[i] = N/float(count[i]) * 3\n",
    "        elif i ==9:\n",
    "            weight_per_class[i] = N/float(count[i]) * 8.0\n",
    "        else:\n",
    "            weight_per_class[i] = N/float(count[i])\n",
    "\n",
    "    print(weight_per_class)                              \n",
    "    weight = [0] * len(images)                                              \n",
    "    for idx, val in enumerate(images):                                          \n",
    "        weight[idx] = weight_per_class[val]                                  \n",
    "    return weight    \n",
    "train_dataset1 = My_dataset(train_data, train_label)\n",
    "weights = make_weights_for_balanced_classes(train_dataset1.label, 12)\n",
    "weights = torch.DoubleTensor(weights)                                     \n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))    \n",
    "train_dataloader1 = DataLoader(train_dataset1, batch_size=1024, sampler=sampler)\n",
    "valid_dataset1 = My_dataset(valid_data, valid_label)\n",
    "valid_dataloader1 = DataLoader(valid_dataset1, batch_size=1, shuffle=False)\n",
    "test_dataset1 = My_dataset(test_data, test_label)\n",
    "test_dataloader1 = DataLoader(test_dataset1, batch_size=1, shuffle=False)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, reduction='mean', gamma=0, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.ce = torch.nn.CrossEntropyLoss(weight=weight, reduction=reduction)\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()\n",
    "\n",
    "class My_module(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super(My_module, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(400)\n",
    "        self.fc1 = nn.Linear(400, 256) \n",
    "        self.fc2 = nn.Linear(256, n_class)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.loss = FocalLoss(gamma=2)\n",
    "    def forward(self, x, y=None):\n",
    "        # x = torch.nn.functional.dropout(x, p=0.5)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.dropout(x, p=0.5)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        if y is not None:\n",
    "            loss = self.loss(x, y)\n",
    "            return loss\n",
    "        else:\n",
    "            return x\n",
    "network1 = My_module(n_class=12)\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_counter = [i * len(train_dataloader1.dataset) for i in range(20 + 1)]\n",
    "epochs = 500\n",
    "max_acc = 0\n",
    "def train(epochs,network,train_dataloader,valid_dataloader, lr):\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=lr, momentum=0.9)  \n",
    "    max_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        all_pred1 = []\n",
    "        all_target1 = []\n",
    "        network.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            loss = network(data, target)\n",
    "            loss = loss.mean()  # 并行会算出一组loss，需要进行平均才能backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            output = network(data)\n",
    "            preds = torch.argmax(output, dim=-1)\n",
    "            if batch_idx % 40 == 0:\n",
    "                print('Train Epoch: {}[{}/{}({:.0f}%)]\\tloss:{:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
    "                        40. * batch_idx / len(train_dataloader), loss.item()))\n",
    "                train_losses.append(loss.item())\n",
    "            if len(all_pred1) == 0:\n",
    "                all_pred1.append(preds.detach().cpu().numpy())\n",
    "                all_target1.append(target.detach().cpu().numpy())\n",
    "            else:\n",
    "                all_pred1[0] = np.append(all_pred1[0], preds.detach().cpu().numpy(), axis=0)\n",
    "                all_target1[0] = np.append(all_target1[0], target.detach().cpu().numpy(), axis=0)\n",
    "        #     print(all_pred)\n",
    "        all_pred1, all_target1 = all_pred1[0], all_target1[0]\n",
    "        \n",
    "        acc = (all_pred1 == all_target1).mean()\n",
    "        print('train: %.4f' % (acc))\n",
    "        test(epoch,network,valid_dataloader)\n",
    "\n",
    "def test(epoch,network,valid_dataloader):\n",
    "    global max_acc\n",
    "    all_pred = []\n",
    "    all_target = []\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_dataloader:\n",
    "            output = network(data)\n",
    "            output = torch.log_softmax(output, dim=-1)\n",
    "            # test_loss = torch.nn.functional.nll_loss(output, target)\n",
    "            preds = torch.argmax(output, dim=-1)\n",
    "            if len(all_pred) == 0:\n",
    "                all_pred.append(preds.detach().cpu().numpy())\n",
    "                all_target.append(target.detach().cpu().numpy())\n",
    "            else:\n",
    "                all_pred[0] = np.append(all_pred[0], preds.detach().cpu().numpy(), axis=0)\n",
    "                all_target[0] = np.append(all_target[0], target.detach().cpu().numpy(), axis=0)\n",
    "    #     print(all_pred)\n",
    "    all_pred, all_target = all_pred[0], all_target[0]\n",
    "    t = classification_report(all_target, all_pred)\n",
    "    acc = (all_pred == all_target).mean()\n",
    "    if acc > max_acc:\n",
    "        max_acc = acc\n",
    "        print(\"save model, and max acc is: %.4f\" %(acc))\n",
    "        torch.save(network.state_dict(), 'best_model.pth')\n",
    "    print('Testing Accuracy : %.4f' % (acc))\n",
    "    print(t)\n",
    "    return all_pred\n",
    "lr = 0.01\n",
    "train(epochs,network1,train_dataloader1,valid_dataloader1,lr)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
